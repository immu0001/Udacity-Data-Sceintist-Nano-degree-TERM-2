{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/imran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/imran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/imran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-07e014443159>\", line 119, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-2-07e014443159>\", line 114, in main\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 710, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 1151, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 689, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 515, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 350, in fit\n",
      "    Xt, fit_params = self._fit(X, y, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 315, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/memory.py\", line 355, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 728, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 936, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 966, in _parallel_func\n",
      "    weight) in enumerate(transformers, 1))\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 1004, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 728, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 385, in fit_transform\n",
      "    Xt, fit_params = self._fit(X, y, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 315, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/joblib/memory.py\", line 355, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 728, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1220, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1131, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 105, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "  File \"<ipython-input-2-07e014443159>\", line 53, in tokenize\n",
      "    tokens = word_tokenize(text)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\", line 146, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\", line 146, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\", line 143, in tokenize\n",
      "    text = regexp.sub(r' \\1 \\2 ', text)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/posixpath.py\", line 396, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/home/imran/anaconda3/lib/python3.7/posixpath.py\", line 378, in abspath\n",
      "    path = os.fspath(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4],\n",
    "        'features__transformer_weights': (\n",
    "            {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "            {'text_pipeline': 0.5, 'starting_verb': 1},\n",
    "            {'text_pipeline': 0.8, 'starting_verb': 1},\n",
    "        )\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
